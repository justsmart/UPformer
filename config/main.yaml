DATA:
  test_root:  ../indusData/TestDataset #../indusData/TestDataset  or dataset/TestDataset
  train_root: ../indusData/TrainDataset
  classes: 1
  new: True
TRAIN:
  num_clusters: 32
  mode: #cluster  # clustering before training
  threads: 4  
  cacheBatchSize: 16
  arch: UPformer
  layers: 50
  sync_bn: False  # adopt sync_bn or not
  train_h: 353
  train_w: 353
  scale_min: 0.5  # minimum random scale
  scale_max: 2.0  # maximum random scale
  rotate_min: -90 #-10  # minimum random rotate
  rotate_max: 90 #10  # maximum random rotate
  zoom_factor: 8  # zoom factor for final prediction during training, be in [1, 2, 4, 8]
  ignore_label: 0 # 255
  aux_weight: 0.4
  train_gpu: [1]
  workers: 4  # data loader workers
  batch_size: 8 #12  # batch size for training
  batch_size_val: 36  # no usage
  base_lr: 0.01   #0.01 for poloy 0.001 for indus
  epochs: 1000 #500 #200 #50
  start_epoch: 0
  power: 0.9
  momentum: 0.9
  weight_decay: 0.0001
  manual_seed:
  print_freq: 20
  save_freq: 100
  save_path: model_file 
  weight:  pre_trained/resnet50_v1.pth
  resume:
  evaluate:  True #False  # evaluate on validation set, extra gpu memory needed and small batch_size_val is recommend

Distributed: # Distributed is unavailable
  dist_url: tcp://127.0.0.1:6789
  dist_backend: 'nccl'
  multiprocessing_distributed: False # Distributed is unavailable, set false
  world_size: 1
  rank: 0
  use_apex: False
  opt_level: 'O0'
  keep_batchnorm_fp32:
  loss_scale:

TEST:
  split: test  # split in [train, val and test]
  base_size: 353  # based size for scaling
  test_h: 353
  test_w: 353
  scales: [1.0]  # evaluation scales, ms as [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]
  test_batch_size: 1
  has_prediction: False  # has prediction already or not
  index_start: 0  # evaluation start index in list
  index_step: 0  # evaluation step index in list, 0 means to end
  test_gpu: [1]
  model_root:  /home/liucl/PycharmProjects/nosupAD/UPformer/saved_modelfile/indus1000-new
  save_folder: ./result/  # results save folder

